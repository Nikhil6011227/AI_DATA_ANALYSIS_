{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    EmployeeID     Name   Age                Email           Phone\n",
      "0         101    Alice  25.0    alice@example.com  (123) 456-7890\n",
      "1         102      Bob  30.0      bob@example.com      1234567890\n",
      "2         103  Charlie  -1.0        invalid_email  (987) 654-3210\n",
      "3         103  Charlie  28.0  charlie@example.com            None\n",
      "4         105      Eve   NaN          eve@example  (123)-456-7890\n",
      "\n",
      "=== Data Profiling Summary ===\n",
      "        EmployeeID     Name        Age              Email           Phone\n",
      "count      5.00000        5   4.000000                  5               4\n",
      "unique         NaN        4        NaN                  5               4\n",
      "top            NaN  Charlie        NaN  alice@example.com  (123) 456-7890\n",
      "freq           NaN        2        NaN                  1               1\n",
      "mean     102.80000      NaN  20.500000                NaN             NaN\n",
      "std        1.48324      NaN  14.479871                NaN             NaN\n",
      "min      101.00000      NaN  -1.000000                NaN             NaN\n",
      "25%      102.00000      NaN  18.500000                NaN             NaN\n",
      "50%      103.00000      NaN  26.500000                NaN             NaN\n",
      "75%      103.00000      NaN  28.500000                NaN             NaN\n",
      "max      105.00000      NaN  30.000000                NaN             NaN\n",
      "\n",
      "Missing Values:\n",
      " EmployeeID    0\n",
      "Name          0\n",
      "Age           1\n",
      "Email         0\n",
      "Phone         1\n",
      "dtype: int64\n",
      "\n",
      "=== Rule-based Quality Checks ===\n",
      "\n",
      "Rule: Unique EmployeeID\n",
      "   EmployeeID     Name   Age                Email           Phone\n",
      "2         103  Charlie  -1.0        invalid_email  (987) 654-3210\n",
      "3         103  Charlie  28.0  charlie@example.com            None\n",
      "\n",
      "Rule: Age between 18 and 65\n",
      "   EmployeeID     Name  Age          Email           Phone\n",
      "2         103  Charlie -1.0  invalid_email  (987) 654-3210\n",
      "4         105      Eve  NaN    eve@example  (123)-456-7890\n",
      "\n",
      "Rule: Valid Email Format\n",
      "   EmployeeID     Name  Age          Email           Phone\n",
      "2         103  Charlie -1.0  invalid_email  (987) 654-3210\n",
      "4         105      Eve  NaN    eve@example  (123)-456-7890\n",
      "\n",
      "Rule: Valid Phone Number Format (e.g., (123) 456-7890)\n",
      "   EmployeeID     Name   Age                Email           Phone\n",
      "1         102      Bob  30.0      bob@example.com      1234567890\n",
      "3         103  Charlie  28.0  charlie@example.com            None\n",
      "4         105      Eve   NaN          eve@example  (123)-456-7890\n",
      "\n",
      "=== Cleaned Dataset Suggestion ===\n",
      "   EmployeeID     Name   Age              Email           Phone  Email_Valid  \\\n",
      "0         101    Alice  25.0  alice@example.com  (123) 456-7890         True   \n",
      "1         102      Bob  30.0    bob@example.com      1234567890         True   \n",
      "2         103  Charlie   NaN      invalid_email  (987) 654-3210        False   \n",
      "4         105      Eve   NaN        eve@example  (123)-456-7890        False   \n",
      "\n",
      "   Phone_Valid  \n",
      "0         True  \n",
      "1        False  \n",
      "2         True  \n",
      "4        False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_468/3335095761.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Age'] = df_cleaned['Age'].apply(lambda x: np.nan if x < 18 or x > 65 else x)  # Fix Age\n",
      "/tmp/ipykernel_468/3335095761.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Email_Valid'] = df_cleaned['Email'].str.match(email_pattern)\n",
      "/tmp/ipykernel_468/3335095761.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Phone_Valid'] = df_cleaned['Phone'].str.match(phone_pattern)\n"
     ]
    }
   ],
   "source": [
    "# Task B: Using DQ Labs\n",
    "\n",
    "# 22. Tool Setup and Configuration:\n",
    "# - Download and configure DQ Labs on your local environment.\n",
    "# - Create a new data quality project.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 23. Data Analysis Automation:\n",
    "# - Apply DQ Labs for automating data profiling and quality checks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 24. Quality Rule Creation:\n",
    "# - Create quality rules for detecting and handling duplicates or enforcing standards.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Sample dataset (simulating uploaded data in DQ Labs)\n",
    "data = {\n",
    "    'EmployeeID': [101, 102, 103, 103, 105],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Charlie', 'Eve'],\n",
    "    'Age': [25, 30, -1, 28, None],\n",
    "    'Email': ['alice@example.com', 'bob@example.com', 'invalid_email', 'charlie@example.com', 'eve@example'],\n",
    "    'Phone': ['(123) 456-7890', '1234567890', '(987) 654-3210', None, '(123)-456-7890']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original Data:\\n\", df)\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Data Profiling Summary\n",
    "# -------------------------------\n",
    "print(\"\\n=== Data Profiling Summary ===\")\n",
    "print(df.describe(include='all'))\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Rule-based Validation\n",
    "# -------------------------------\n",
    "print(\"\\n=== Rule-based Quality Checks ===\")\n",
    "\n",
    "# Rule 1: EmployeeID must be unique\n",
    "duplicate_ids = df['EmployeeID'].duplicated(keep=False)\n",
    "print(\"\\nRule: Unique EmployeeID\")\n",
    "print(df[duplicate_ids])\n",
    "\n",
    "# Rule 2: Age must be between 18 and 65\n",
    "invalid_age = ~df['Age'].between(18, 65, inclusive='both')\n",
    "print(\"\\nRule: Age between 18 and 65\")\n",
    "print(df[invalid_age])\n",
    "\n",
    "# Rule 3: Email should follow proper format\n",
    "email_pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\n",
    "invalid_email = ~df['Email'].astype(str).str.match(email_pattern)\n",
    "print(\"\\nRule: Valid Email Format\")\n",
    "print(df[invalid_email])\n",
    "\n",
    "# Rule 4: Phone number format (US Style: (XXX) XXX-XXXX)\n",
    "phone_pattern = r'^\\(\\d{3}\\)\\s\\d{3}-\\d{4}$'\n",
    "invalid_phone = ~df['Phone'].astype(str).str.match(phone_pattern)\n",
    "print(\"\\nRule: Valid Phone Number Format (e.g., (123) 456-7890)\")\n",
    "print(df[invalid_phone])\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Suggest Cleaned Dataset\n",
    "# -------------------------------\n",
    "print(\"\\n=== Cleaned Dataset Suggestion ===\")\n",
    "\n",
    "df_cleaned = df.drop_duplicates(subset=['EmployeeID'])  # Remove ID duplicates\n",
    "df_cleaned['Age'] = df_cleaned['Age'].apply(lambda x: np.nan if x < 18 or x > 65 else x)  # Fix Age\n",
    "df_cleaned['Email_Valid'] = df_cleaned['Email'].str.match(email_pattern)\n",
    "df_cleaned['Phone_Valid'] = df_cleaned['Phone'].str.match(phone_pattern)\n",
    "\n",
    "print(df_cleaned)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataContextError",
     "evalue": "Datasource is not a FluentDatasource",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataContextError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m context \u001b[38;5;241m=\u001b[39m ge\u001b[38;5;241m.\u001b[39mget_context()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas_datasource\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m context\u001b[38;5;241m.\u001b[39mlist_datasources()]:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_datasource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpandas_datasource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPandasDatasource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Step 3: Create or load expectation suite\u001b[39;00m\n\u001b[1;32m     22\u001b[0m suite_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memployee_data_suite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/great_expectations/data_context/data_context/abstract_data_context.py:718\u001b[0m, in \u001b[0;36mAbstractDataContext.add_datasource\u001b[0;34m(self, name, initialize, datasource, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;129m@new_argument\u001b[39m(\n\u001b[1;32m    692\u001b[0m     argument_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasource\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    693\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.15.49\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FluentDatasource \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add a new Datasource to the data context, with configuration provided as kwargs.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    --Documentation--\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m        Datasource instance added.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_datasource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitialize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatasource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/great_expectations/data_context/data_context/abstract_data_context.py:758\u001b[0m, in \u001b[0;36mAbstractDataContext._add_datasource\u001b[0;34m(self, name, initialize, datasource, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_fluent_datasource(\n\u001b[1;32m    755\u001b[0m         datasource\u001b[38;5;241m=\u001b[39mdatasource,\n\u001b[1;32m    756\u001b[0m     )\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataContextError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasource is not a FluentDatasource\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# noqa: TRY003 # FIXME CoP\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datasource\n",
      "\u001b[0;31mDataContextError\u001b[0m: Datasource is not a FluentDatasource"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "\n",
    "# Step 1: Create sample data\n",
    "data = {\n",
    "    \"EmployeeID\": [101, 102, 103, 104, 105],\n",
    "    \"Age\": [25, 30, 28, -1, 45],  # -1 is invalid\n",
    "    \"Email\": [\"a@x.com\", \"b@x.com\", \"invalid\", \"d@x.com\", \"e@x.com\"],\n",
    "    \"Salary\": [5000, 6000, None, 7000, 8000]  # None is invalid (null)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Get GE context and add pandas datasource if not present\n",
    "context = ge.get_context()\n",
    "\n",
    "if \"pandas_datasource\" not in [ds[\"name\"] for ds in context.list_datasources()]:\n",
    "    context.add_datasource(name=\"pandas_datasource\", class_name=\"PandasDatasource\")\n",
    "\n",
    "# Step 3: Create or load expectation suite\n",
    "suite_name = \"employee_data_suite\"\n",
    "try:\n",
    "    suite = context.get_expectation_suite(suite_name)\n",
    "except ge.exceptions.DataContextError:\n",
    "    suite = context.create_expectation_suite(suite_name)\n",
    "\n",
    "# Step 4: Create batch request from runtime DataFrame\n",
    "batch_request = RuntimeBatchRequest(\n",
    "    datasource_name=\"pandas_datasource\",\n",
    "    data_connector_name=\"default_runtime_data_connector_name\",\n",
    "    data_asset_name=\"employee_data_asset\",\n",
    "    runtime_parameters={\"batch_data\": df},\n",
    "    batch_identifiers={\"default_identifier_name\": \"default_identifier\"},\n",
    ")\n",
    "\n",
    "# Step 5: Get validator and add expectations\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=suite_name,\n",
    ")\n",
    "\n",
    "validator.expect_column_values_to_be_between(\"Age\", min_value=0, max_value=100)\n",
    "validator.expect_column_values_to_not_be_null(\"Salary\")\n",
    "validator.expect_column_values_to_match_regex(\"Email\", r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\")\n",
    "\n",
    "# Save expectations to suite\n",
    "validator.save_expectation_suite()\n",
    "\n",
    "# Step 6: Validate data and print results\n",
    "results = validator.validate()\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "print(results)\n",
    "\n",
    "# Step 7 (Optional): Print failed expectations details\n",
    "for res in results[\"results\"]:\n",
    "    if not res[\"success\"]:\n",
    "        print(\"\\n❌ Failed Expectation:\")\n",
    "        print(res[\"expectation_config\"][\"expectation_type\"])\n",
    "        print(\"Details:\", res[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
